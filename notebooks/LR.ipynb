{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST8OTzXIUgtp"
      },
      "source": [
        "Unigram by just considering the review text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEushl0VUmFX",
        "outputId": "eddef124-e0c8-4aac-cda0-dad527dcca6c"
      },
      "source": [
        "!pip install nltk==3.4\n",
        "import csv                              \n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from random import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.4 in /usr/local/lib/python3.7/dist-packages (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4) (1.15.0)\n",
            "Requirement already satisfied: singledispatch in /usr/local/lib/python3.7/dist-packages (from nltk==3.4) (3.6.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-aZChcvUtRv"
      },
      "source": [
        "# load data from a file and append it to the rawData\n",
        "def loadData(path, Text=None):\n",
        "    with open(path) as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        next(reader)\n",
        "        for line in reader:\n",
        "            (Id, Text, Label) = parseReview(line)\n",
        "            rawData.append((Id, Text, Label))\n",
        "            preprocessedData.append((Id, preProcess(Text), Label))\n",
        "        \n",
        "def splitData(percentage):\n",
        "    dataSamples = len(rawData)\n",
        "    halfOfData = int(len(rawData)/2)\n",
        "    trainingSamples = int((percentage*dataSamples)/2)\n",
        "    for (_, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
        "        trainData.append((toFeatureVector(preProcess(Text)),Label))\n",
        "    for (_, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
        "        testData.append((toFeatureVector(preProcess(Text)),Label))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i7p3Dd_VFMf"
      },
      "source": [
        "# Convert line from input file into an id/text/label tuple\n",
        "def parseReview(reviewLine):\n",
        "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
        "    s=\"\"\n",
        "    if reviewLine[1]==\"__label1__\":\n",
        "        s = \"fake\"\n",
        "    else: \n",
        "        s = \"real\"\n",
        "    return (reviewLine[0], reviewLine[8], s)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg-Y2M1rVMNI"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.util import ngrams\n",
        "import string"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83oGUVFMVNUZ"
      },
      "source": [
        "\n",
        "table = str.maketrans({key: None for key in string.punctuation})\n",
        "\n",
        "def preProcess(text):\n",
        "    # Should return a list of tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    filtered_tokens=[]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.translate(table)\n",
        "    for w in text.split(\" \"):\n",
        "        if w not in stop_words:\n",
        "            filtered_tokens.append(lemmatizer.lemmatize(w.lower()))\n",
        "    return filtered_tokens"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y7WhK7zVSV3"
      },
      "source": [
        "featureDict = {} # A global dictionary of features\n",
        "\n",
        "def toFeatureVector(tokens):\n",
        "    localDict = {}\n",
        "    for token in tokens:\n",
        "        if token not in featureDict:\n",
        "            featureDict[token] = 1\n",
        "        else:\n",
        "            featureDict[token] = +1\n",
        "   \n",
        "        if token not in localDict:\n",
        "            localDict[token] = 1\n",
        "        else:\n",
        "            localDict[token] = +1\n",
        "    \n",
        "    return localDict"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0XJzYUbVaAp"
      },
      "source": [
        "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
        "def trainClassifier(trainData):\n",
        "    print(\"Training Classifier...\")\n",
        "    pipeline =  Pipeline([('clf', LogisticRegression(max_iter=10000))])\n",
        "    return SklearnClassifier(pipeline).train(trainData)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg8bs78-Vfmb"
      },
      "source": [
        "def crossValidate(dataset, folds):\n",
        "    shuffle(dataset)\n",
        "    cv_results = []\n",
        "    foldSize = int(len(dataset)/folds)\n",
        "    for i in range(0,len(dataset),foldSize):\n",
        "        classifier = trainClassifier(dataset[:i]+dataset[foldSize+i:])\n",
        "        y_pred = predictLabels(dataset[i:i+foldSize],classifier)\n",
        "        a = accuracy_score(list(map(lambda d : d[1], dataset[i:i+foldSize])), y_pred)\n",
        "        (p,r,f,_) = precision_recall_fscore_support(list(map(lambda d : d[1], dataset[i:i+foldSize])), y_pred, average ='macro')\n",
        "        #print(a,p,r,f)\n",
        "        cv_results.append((a,p,r,f))\n",
        "    cv_results = (np.mean(np.array(cv_results),axis=0))\n",
        "    return cv_results"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0gyM_WwVjWh"
      },
      "source": [
        "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
        "\n",
        "def predictLabels(reviewSamples, classifier):\n",
        "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
        "\n",
        "def predictLabel(reviewSample, classifier):\n",
        "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfmo1RvkVmIz",
        "outputId": "c3acbf95-7409-4e66-fbaf-d4530b9ea19c"
      },
      "source": [
        "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
        "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
        "trainData = []        # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
        "testData = []         # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
        "\n",
        "# the output classes\n",
        "fakeLabel = 'fake'\n",
        "realLabel = 'real'\n",
        "\n",
        "# references to the data files\n",
        "reviewPath = 'amazon_reviews.txt'\n",
        "\n",
        "## Do the actual stuff\n",
        "# We parse the dataset and put it in a raw data list\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Preparing the dataset...\",sep='\\n')\n",
        "loadData(reviewPath) \n",
        "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Preparing training and test data...\",sep='\\n')\n",
        "splitData(0.8)\n",
        "# We print the number of training samples and the number of features\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
        "print(\"Mean of cross-validations (Accuracy, Precision, Recall, Fscore): \", crossValidate(trainData, 10))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now 0 rawData, 0 trainData, 0 testData\n",
            "Preparing the dataset...\n",
            "Now 21000 rawData, 0 trainData, 0 testData\n",
            "Preparing training and test data...\n",
            "Now 21000 rawData, 16800 trainData, 4200 testData\n",
            "Training Samples: \n",
            "16800\n",
            "Features: \n",
            "42400\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Mean of cross-validations (Accuracy, Precision, Recall, Fscore):  [0.66166667 0.66191129 0.66170646 0.6614714 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP5cQkkPV2Kd",
        "outputId": "0848b4da-2d64-441e-9759-9f8b38fd6fce"
      },
      "source": [
        "#  TEST DATA\n",
        "classifier = trainClassifier(trainData)\n",
        "predictions = predictLabels(testData, classifier)\n",
        "true_labels = list(map(lambda d: d[1], testData))\n",
        "a = accuracy_score(true_labels, predictions)\n",
        "p, r, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
        "print(\"accuracy: \", a)\n",
        "print(\"Precision: \", p)\n",
        "print(\"Recall: \", a)\n",
        "print(\"f1-score: \", f1)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier...\n",
            "accuracy:  0.6307142857142857\n",
            "Precision:  0.6307192951385212\n",
            "Recall:  0.6307142857142857\n",
            "f1-score:  0.6307107477390232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kn40mGiV7AN"
      },
      "source": [
        "Unigram by considering some other features as well "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znDd55M7V_cr"
      },
      "source": [
        "# load data from a file and append it to the rawData\n",
        "def loadData(path, Text=None):\n",
        "    with open(path) as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        next(reader)\n",
        "        for line in reader:\n",
        "            (Id, Rating, verified_Purchase, product_Category, Text, Label) = parseReview(line)\n",
        "            rawData.append((Id, Rating, verified_Purchase, product_Category, Text, Label))\n",
        "            #preprocessedData.append((Id, preProcess(Text), Label))\n",
        "        \n",
        "def splitData(percentage):\n",
        "    dataSamples = len(rawData)\n",
        "    halfOfData = int(len(rawData)/2)\n",
        "    trainingSamples = int((percentage*dataSamples)/2)\n",
        "    for (_, Rating, verified_Purchase, product_Category, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
        "        trainData.append((toFeatureVector(Rating, verified_Purchase, product_Category, preProcess(Text)),Label))\n",
        "    for (_, Rating, verified_Purchase, product_Category, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
        "        testData.append((toFeatureVector(Rating, verified_Purchase, product_Category, preProcess(Text)),Label))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYlosjzkWWUI"
      },
      "source": [
        "# QUESTION 1\n",
        "\n",
        "# Convert line from input file into an id/text/label tuple\n",
        "def parseReview(reviewLine):\n",
        "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
        "    s=\"\"\n",
        "    if reviewLine[1]==\"__label1__\":\n",
        "        s = \"fake\"\n",
        "    else: \n",
        "        s = \"real\"\n",
        "    return (reviewLine[0], reviewLine[2], reviewLine[3],reviewLine[4], reviewLine[8], s)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj43fX_fWgKg"
      },
      "source": [
        "featureDict = {} # A global dictionary of features\n",
        "\n",
        "def toFeatureVector(Rating, verified_Purchase, product_Category, tokens):\n",
        "    localDict = {}\n",
        "    \n",
        "#Rating\n",
        "\n",
        "    #print(Rating)\n",
        "    featureDict[\"R\"] = 1   \n",
        "    localDict[\"R\"] = Rating\n",
        "\n",
        "#Verified_Purchase\n",
        "  \n",
        "    featureDict[\"VP\"] = 1\n",
        "            \n",
        "    if verified_Purchase == \"N\":\n",
        "        localDict[\"VP\"] = 0\n",
        "    else:\n",
        "        localDict[\"VP\"] = 1\n",
        "\n",
        "#Product_Category\n",
        "\n",
        "    \n",
        "    if product_Category not in featureDict:\n",
        "        featureDict[product_Category] = 1\n",
        "    else:\n",
        "        featureDict[product_Category] = +1\n",
        "            \n",
        "    if product_Category not in localDict:\n",
        "        localDict[product_Category] = 1\n",
        "    else:\n",
        "        localDict[product_Category] = +1\n",
        "            \n",
        "            \n",
        "#Text        \n",
        "\n",
        "    for token in tokens:\n",
        "        if token not in featureDict:\n",
        "            featureDict[token] = 1\n",
        "        else:\n",
        "            featureDict[token] = +1\n",
        "            \n",
        "        if token not in localDict:\n",
        "            localDict[token] = 1\n",
        "        else:\n",
        "            localDict[token] = +1\n",
        "    \n",
        "    return localDict"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBGG74m3Wi79",
        "outputId": "c2962216-836c-4cf8-fb53-4d11e4c7de39"
      },
      "source": [
        "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
        "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
        "trainData = []        # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
        "testData = []         # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
        "\n",
        "# the output classes\n",
        "fakeLabel = 'fake'\n",
        "realLabel = 'real'\n",
        "\n",
        "# references to the data files\n",
        "reviewPath = 'amazon_reviews.txt'\n",
        "\n",
        "## Do the actual stuff\n",
        "# We parse the dataset and put it in a raw data list\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Preparing the dataset...\",sep='\\n')\n",
        "loadData(reviewPath) \n",
        "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Preparing training and test data...\",sep='\\n')\n",
        "splitData(0.8)\n",
        "# We print the number of training samples and the number of features\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
        "print(\"Mean of cross-validations (Accuracy, Precision, Recall, Fscore): \", crossValidate(trainData, 10))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now 0 rawData, 0 trainData, 0 testData\n",
            "Preparing the dataset...\n",
            "Now 21000 rawData, 0 trainData, 0 testData\n",
            "Preparing training and test data...\n",
            "Now 21000 rawData, 16800 trainData, 4200 testData\n",
            "Training Samples: \n",
            "16800\n",
            "Features: \n",
            "42432\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Mean of cross-validations (Accuracy, Precision, Recall, Fscore):  [0.79916667 0.79930573 0.79931131 0.79912941]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DChAAgygWunp",
        "outputId": "62bafb69-1e02-4197-89e9-76ffe03ccfa3"
      },
      "source": [
        "#  TEST DATA\n",
        "classifier = trainClassifier(trainData)\n",
        "predictions = predictLabels(testData, classifier)\n",
        "true_labels = list(map(lambda d: d[1], testData))\n",
        "a = accuracy_score(true_labels, predictions)\n",
        "p, r, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
        "print(\"accuracy: \", a)\n",
        "print(\"Precision: \", p)\n",
        "print(\"Recall: \", a)\n",
        "print(\"f1-score: \", f1)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier...\n",
            "accuracy:  0.7702380952380953\n",
            "Precision:  0.7718760715065436\n",
            "Recall:  0.7702380952380953\n",
            "f1-score:  0.7698915106318581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oniBU8gWyPt"
      },
      "source": [
        "Bigram + Text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiAhBqztW3Zh"
      },
      "source": [
        "def loadData(path, Text=None):\n",
        "    with open(path) as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        next(reader)\n",
        "        for line in reader:\n",
        "            (Id, Text, Label) = parseReview(line)\n",
        "            rawData.append((Id, Text, Label))\n",
        "            preprocessedData.append((Id, preProcess(Text), Label))\n",
        "        \n",
        "def splitData(percentage):\n",
        "    dataSamples = len(rawData)\n",
        "    halfOfData = int(len(rawData)/2)\n",
        "    trainingSamples = int((percentage*dataSamples)/2)\n",
        "    for (_, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
        "        trainData.append((toFeatureVector(preProcess(Text)),Label))\n",
        "    for (_, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
        "        testData.append((toFeatureVector(preProcess(Text)),Label))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHHgenCzW7gs"
      },
      "source": [
        "def parseReview(reviewLine):\n",
        "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
        "    s=\"\"\n",
        "    if reviewLine[1]==\"__label1__\":\n",
        "        s = \"fake\"\n",
        "    else: \n",
        "        s = \"real\"\n",
        "    return (reviewLine[0], reviewLine[8], s)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2LoDM_nXVJs"
      },
      "source": [
        "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
        "# Input: a string of one review\n",
        "table = str.maketrans({key: None for key in string.punctuation})\n",
        "def preProcess(text):\n",
        "    # Should return a list of tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    filtered_tokens=[]\n",
        "    lemmatized_tokens = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.translate(table)\n",
        "    for w in text.split(\" \"):\n",
        "        if w not in stop_words:\n",
        "            lemmatized_tokens.append(lemmatizer.lemmatize(w.lower()))\n",
        "        filtered_tokens = [' '.join(l) for l in nltk.bigrams(lemmatized_tokens)] + lemmatized_tokens\n",
        "    return filtered_tokens"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNsxNeIYXIZQ"
      },
      "source": [
        "featureDict = {} # A global dictionary of features\n",
        "\n",
        "def toFeatureVector(tokens):\n",
        "    localDict = {}\n",
        "    for token in tokens:\n",
        "        if token not in featureDict:\n",
        "            featureDict[token] = 1\n",
        "        else:\n",
        "            featureDict[token] = +1\n",
        "   \n",
        "        if token not in localDict:\n",
        "            localDict[token] = 1\n",
        "        else:\n",
        "            localDict[token] = +1\n",
        "    \n",
        "    return localDict"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAdqEKTqXYOa",
        "outputId": "d5bb6491-2456-48c0-b16f-9e4b1ab500a6"
      },
      "source": [
        "# MAIN\n",
        "\n",
        "# loading reviews\n",
        "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
        "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
        "trainData = []        # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
        "testData = []         # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
        "\n",
        "# the output classes\n",
        "fakeLabel = 'fake'\n",
        "realLabel = 'real'\n",
        "\n",
        "# references to the data files\n",
        "reviewPath = 'amazon_reviews.txt'\n",
        "\n",
        "## Do the actual stuff\n",
        "# We parse the dataset and put it in a raw data list\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Preparing the dataset...\",sep='\\n')\n",
        "loadData(reviewPath) \n",
        "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Preparing training and test data...\",sep='\\n')\n",
        "splitData(0.8)\n",
        "# We print the number of training samples and the number of features\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
        "print(\"Mean of cross-validations (Accuracy, Precision, Recall, Fscore): \", crossValidate(trainData, 10))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now 0 rawData, 0 trainData, 0 testData\n",
            "Preparing the dataset...\n",
            "Now 21000 rawData, 0 trainData, 0 testData\n",
            "Preparing training and test data...\n",
            "Now 21000 rawData, 16800 trainData, 4200 testData\n",
            "Training Samples: \n",
            "16800\n",
            "Features: \n",
            "512213\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Mean of cross-validations (Accuracy, Precision, Recall, Fscore):  [0.68279762 0.68308209 0.6827567  0.6825689 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4jO0qVJYJnK",
        "outputId": "bfd5fa7c-0ff0-49ab-a156-0b1fe490d026"
      },
      "source": [
        "#  TEST DATA\n",
        "classifier = trainClassifier(trainData)\n",
        "predictions = predictLabels(testData, classifier)\n",
        "true_labels = list(map(lambda d: d[1], testData))\n",
        "a = accuracy_score(true_labels, predictions)\n",
        "p, r, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
        "print(\"accuracy: \", a)\n",
        "print(\"Precision: \", p)\n",
        "print(\"Recall: \", a)\n",
        "print(\"f1-score: \", f1)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier...\n",
            "accuracy:  0.6545238095238095\n",
            "Precision:  0.6546222979671178\n",
            "Recall:  0.6545238095238095\n",
            "f1-score:  0.6544687870080899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxMlC9EmYBao"
      },
      "source": [
        "Bigram + Additional Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRGQ4q6-YD1h"
      },
      "source": [
        "# load data from a file and append it to the rawData\n",
        "def loadData(path, Text=None):\n",
        "    with open(path) as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        next(reader)\n",
        "        for line in reader:\n",
        "            (Id, Rating, verified_Purchase, product_Category, Text, Label) = parseReview(line)\n",
        "            rawData.append((Id, Rating, verified_Purchase, product_Category, Text, Label))\n",
        "            #preprocessedData.append((Id, preProcess(Text), Label))\n",
        "        \n",
        "def splitData(percentage):\n",
        "    dataSamples = len(rawData)\n",
        "    halfOfData = int(len(rawData)/2)\n",
        "    trainingSamples = int((percentage*dataSamples)/2)\n",
        "    for (_, Rating, verified_Purchase, product_Category, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
        "        trainData.append((toFeatureVector(Rating, verified_Purchase, product_Category, preProcess(Text)),Label))\n",
        "    for (_, Rating, verified_Purchase, product_Category, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
        "        testData.append((toFeatureVector(Rating, verified_Purchase, product_Category, preProcess(Text)),Label))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CVBB2TJYdj6"
      },
      "source": [
        "# QUESTION 1\n",
        "\n",
        "# Convert line from input file into an id/text/label tuple\n",
        "def parseReview(reviewLine):\n",
        "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
        "    s=\"\"\n",
        "    if reviewLine[1]==\"__label1__\":\n",
        "        s = \"fake\"\n",
        "    else: \n",
        "        s = \"real\"\n",
        "    return (reviewLine[0], reviewLine[2], reviewLine[3],reviewLine[4], reviewLine[8], s)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXefmk-TYiBa"
      },
      "source": [
        "featureDict = {} # A global dictionary of features\n",
        "\n",
        "def toFeatureVector(Rating, verified_Purchase, product_Category, tokens):\n",
        "    localDict = {}\n",
        "    \n",
        "#Rating\n",
        "\n",
        "    #print(Rating)\n",
        "    featureDict[\"R\"] = 1   \n",
        "    localDict[\"R\"] = Rating\n",
        "\n",
        "#Verified_Purchase\n",
        "  \n",
        "    featureDict[\"VP\"] = 1\n",
        "            \n",
        "    if verified_Purchase == \"N\":\n",
        "        localDict[\"VP\"] = 0\n",
        "    else:\n",
        "        localDict[\"VP\"] = 1\n",
        "\n",
        "#Product_Category\n",
        "\n",
        "    \n",
        "    if product_Category not in featureDict:\n",
        "        featureDict[product_Category] = 1\n",
        "    else:\n",
        "        featureDict[product_Category] = +1\n",
        "            \n",
        "    if product_Category not in localDict:\n",
        "        localDict[product_Category] = 1\n",
        "    else:\n",
        "        localDict[product_Category] = +1\n",
        "            \n",
        "            \n",
        "#Text        \n",
        "\n",
        "    for token in tokens:\n",
        "        if token not in featureDict:\n",
        "            featureDict[token] = 1\n",
        "        else:\n",
        "            featureDict[token] = +1\n",
        "            \n",
        "        if token not in localDict:\n",
        "            localDict[token] = 1\n",
        "        else:\n",
        "            localDict[token] = +1\n",
        "    \n",
        "    return localDict"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X-gGvYoYnAH",
        "outputId": "0bb77f6e-e30c-44ce-c78c-fd288dbd89c0"
      },
      "source": [
        "# MAIN\n",
        "\n",
        "# loading reviews\n",
        "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
        "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
        "trainData = []        # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
        "testData = []         # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
        "\n",
        "# the output classes\n",
        "fakeLabel = 'fake'\n",
        "realLabel = 'real'\n",
        "\n",
        "# references to the data files\n",
        "reviewPath = 'amazon_reviews.txt'\n",
        "\n",
        "## Do the actual stuff\n",
        "# We parse the dataset and put it in a raw data list\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Preparing the dataset...\",sep='\\n')\n",
        "loadData(reviewPath) \n",
        "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Preparing training and test data...\",sep='\\n')\n",
        "splitData(0.8)\n",
        "# We print the number of training samples and the number of features\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
        "print(\"Mean of cross-validations (Accuracy, Precision, Recall, Fscore): \", crossValidate(trainData, 10))\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now 0 rawData, 0 trainData, 0 testData\n",
            "Preparing the dataset...\n",
            "Now 21000 rawData, 0 trainData, 0 testData\n",
            "Preparing training and test data...\n",
            "Now 21000 rawData, 16800 trainData, 4200 testData\n",
            "Training Samples: \n",
            "16800\n",
            "Features: \n",
            "512245\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Mean of cross-validations (Accuracy, Precision, Recall, Fscore):  [0.81815476 0.81867542 0.8181246  0.81803305]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbB_jYWAY91r",
        "outputId": "73b1ecf2-05a3-440a-a5e2-059f970ea437"
      },
      "source": [
        "#  TEST DATA\n",
        "classifier = trainClassifier(trainData)\n",
        "predictions = predictLabels(testData, classifier)\n",
        "true_labels = list(map(lambda d: d[1], testData))\n",
        "a = accuracy_score(true_labels, predictions)\n",
        "p, r, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
        "print(\"accuracy: \", a)\n",
        "print(\"Precision: \", p)\n",
        "print(\"Recall: \", a)\n",
        "print(\"f1-score: \", f1)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier...\n",
            "accuracy:  0.7973809523809524\n",
            "Precision:  0.7999141730228865\n",
            "Recall:  0.7973809523809524\n",
            "f1-score:  0.7969521923022147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91au0R_xZB17"
      },
      "source": [
        "Trigram + Additional Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIlCMnZKZFBP"
      },
      "source": [
        "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
        "# Input: a string of one review\n",
        "table = str.maketrans({key: None for key in string.punctuation})\n",
        "def preProcess(text):\n",
        "    # Should return a list of tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    filtered_tokens=[]\n",
        "    lemmatized_tokens = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.translate(table)\n",
        "    for w in text.split(\" \"):\n",
        "        if w not in stop_words:\n",
        "            lemmatized_tokens.append(lemmatizer.lemmatize(w.lower()))\n",
        "        filtered_tokens = [' '.join(l) for l in nltk.trigrams(lemmatized_tokens)] + lemmatized_tokens\n",
        "    return filtered_tokens"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8YM12UBZT41",
        "outputId": "76b915f7-7889-46ed-a592-e2d848ce5e48"
      },
      "source": [
        "# MAIN\n",
        "\n",
        "# loading reviews\n",
        "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
        "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
        "trainData = []        # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
        "testData = []         # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
        "\n",
        "# the output classes\n",
        "fakeLabel = 'fake'\n",
        "realLabel = 'real'\n",
        "\n",
        "# references to the data files\n",
        "reviewPath = 'amazon_reviews.txt'\n",
        "\n",
        "## Do the actual stuff\n",
        "# We parse the dataset and put it in a raw data list\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Preparing the dataset...\",sep='\\n')\n",
        "loadData(reviewPath) \n",
        "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Preparing training and test data...\",sep='\\n')\n",
        "splitData(0.8)\n",
        "# We print the number of training samples and the number of features\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
        "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
        "print(\"Mean of cross-validations (Accuracy, Precision, Recall, Fscore): \", crossValidate(trainData, 10))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now 0 rawData, 0 trainData, 0 testData\n",
            "Preparing the dataset...\n",
            "Now 21000 rawData, 0 trainData, 0 testData\n",
            "Preparing training and test data...\n",
            "Now 21000 rawData, 16800 trainData, 4200 testData\n",
            "Training Samples: \n",
            "16800\n",
            "Features: \n",
            "1250250\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Training Classifier...\n",
            "Mean of cross-validations (Accuracy, Precision, Recall, Fscore):  [0.81517857 0.81575787 0.81512323 0.81499182]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8095S8cZv1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54823d72-be40-4b95-b88b-c84b9a8259dd"
      },
      "source": [
        "#  TEST DATA\n",
        "classifier = trainClassifier(trainData)\n",
        "predictions = predictLabels(testData, classifier)\n",
        "true_labels = list(map(lambda d: d[1], testData))\n",
        "a = accuracy_score(true_labels, predictions)\n",
        "p, r, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
        "print(\"accuracy: \", a)\n",
        "print(\"Precision: \", p)\n",
        "print(\"Recall: \", a)\n",
        "print(\"f1-score: \", f1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier...\n",
            "accuracy:  0.795952380952381\n",
            "Precision:  0.798743977452801\n",
            "Recall:  0.795952380952381\n",
            "f1-score:  0.7954745868622453\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}